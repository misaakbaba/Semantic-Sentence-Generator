{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important !!\n",
    "## Project Dependencies\n",
    "**zemberek jar file is required**\n",
    "\n",
    "**jpype library is required**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jpype import JString, JClass, getDefaultJVMPath, shutdownJVM, startJVM, addClassPath, java\n",
    "from typing import List\n",
    "import jpype.imports\n",
    "from jpype.types import *\n",
    "import os\n",
    "import string\n",
    "import random\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CreateCorpus Class\n",
    "This class consist of 6 methods:\n",
    "## In Constructor\n",
    "Zemberek package path is defined and jvm is started. Also **TurkishMorphology** class, **AnalysisFormatters** class and **WordAnalysis** class is taken from zemberek.\n",
    "## read_filenames\n",
    "This methods take path of news as an argument and return all files that parent folder consist of\n",
    "## clean_text\n",
    "This method split lines of text and clear text from punctuation\n",
    "## read_files\n",
    "Read all files returned from read_filenames\n",
    "## analyze_word\n",
    "In this method, distinguishes the stem of the given word and determine its type with the help of zemberek\n",
    "## reproduce_noun and reproduce_verb\n",
    "Generating new words from current stem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateCorpus(object):\n",
    "    def __init__(self,zemberek_path):\n",
    "        startJVM(\n",
    "        getDefaultJVMPath(),\n",
    "        '-ea',\n",
    "        f'-Djava.class.path={zemberek_path}',\n",
    "        convertStrings=False\n",
    "    )\n",
    "        self.TurkishMorphology = JClass('zemberek.morphology.TurkishMorphology')\n",
    "        self.AnalysisFormatters = JClass('zemberek.morphology.analysis.AnalysisFormatters')\n",
    "        self.WordAnalysis: JClass = JClass('zemberek.morphology.analysis.WordAnalysis')\n",
    "        self.morphology: self.TurkishMorphology = self.TurkishMorphology.createWithDefaults()\n",
    "    \n",
    "    def read_filenames(self,datapath):\n",
    "        path = os.path.join(os.getcwd(),datapath)\n",
    "        folders_path = [os.path.join(path,item) for item in os.listdir(path)]\n",
    "        ret_list = list()\n",
    "        for folder in folders_path:\n",
    "            file = [os.path.join(folder,file) for file in os.listdir(folder) if file.endswith(\"txt\")]\n",
    "            for item in file:\n",
    "                ret_list.append(item)\n",
    "        return ret_list\n",
    "    \n",
    "    def clean_text(self,text):\n",
    "        words = text.split()\n",
    "        words = [word.lower() for word in words]\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in words]\n",
    "        return stripped\n",
    "    \n",
    "    def read_files(self,filenames):\n",
    "        ret_list = list()\n",
    "        for item in filenames:\n",
    "            file = open(item, 'rt',errors=\"ignore\",encoding='utf-8-sig')\n",
    "            text = file.read()\n",
    "            file.close()\n",
    "            ret_list += self.clean_text(text)\n",
    "        return ret_list\n",
    "    \n",
    "    def analyze_word(self,word):\n",
    "        results: self.WordAnalysis = self.morphology.analyze(JString(word))\n",
    "        if len(list(results)) == 0:\n",
    "            return ()\n",
    "        res = str(list(results)[0].formatLong())\n",
    "        res = res.split(' ')\n",
    "        res = res[1].split('+')\n",
    "        res = res[0].split(':')\n",
    "        word = res[0]\n",
    "        word_type = res[1].split('|')\n",
    "        word_type = word_type[0]\n",
    "        return [word, word_type]\n",
    "        \n",
    "    def reproduce_noun(self,word):\n",
    "        number: List[JString] = [JString('A3sg'), JString('A3pl')]\n",
    "        possessives: List[JString] = [JString('P1sg'), JString('P2sg'), JString('P3sg')]\n",
    "        cases: List[JString] = [JString('Dat'), JString('Loc'), JString('Abl')]\n",
    "            \n",
    "        morphology: self.TurkishMorphology = (\n",
    "        self.TurkishMorphology.builder().setLexicon(word).disableCache().build())\n",
    "        item = morphology.getLexicon().getMatchingItems(word).get(0)\n",
    "        \n",
    "        ret_list = list()\n",
    "        for number_m in number:\n",
    "            for possessive_m in possessives:\n",
    "                for case_m in cases:\n",
    "                    for result in morphology.getWordGenerator().generate(item, number_m, possessive_m, case_m):\n",
    "#                         print(str(result.surface))\n",
    "                        ret_list.append(str(result.surface))\n",
    "        return ret_list\n",
    "    \n",
    "    def reproduce_verb(self,word):\n",
    "        positive_negatives: List[JString] = [JString(''), JString('Neg')]\n",
    "        times: List[JString] = ['Imp', 'Aor', 'Past', 'Prog1', 'Prog2', 'Narr', 'Fut']\n",
    "        people: List[JString] = ['A1sg', 'A2sg', 'A3sg', 'A1pl', 'A2pl', 'A3pl']\n",
    "        \n",
    "        morphology: self.TurkishMorphology = (self.TurkishMorphology.builder().setLexicon(word+'mak').disableCache().build())\n",
    "        stem: str = word\n",
    "            \n",
    "        ret_list = list()\n",
    "        for pos_neg in positive_negatives:\n",
    "            for time in times:\n",
    "                for person in people:\n",
    "                    seq: java.util.ArrayList = java.util.ArrayList()\n",
    "                    if pos_neg:\n",
    "                        seq.add(JString(pos_neg))\n",
    "                    if time:\n",
    "                        seq.add(JString(time))\n",
    "                    if person:\n",
    "                        seq.add(JString(person))\n",
    "                    results = list(morphology.getWordGenerator().generate(JString(stem),seq))\n",
    "                    if not results:\n",
    "                        continue\n",
    "        #             print(' '.join(str(result.surface) for result in results))\n",
    "                    for item in results:\n",
    "#                         print(str(item.surface))\n",
    "                        ret_list.append(str(item.surface))\n",
    "        if len(ret_list) == 0:\n",
    "            morphology: self.TurkishMorphology = (self.TurkishMorphology.builder().setLexicon(word).disableCache().build())\n",
    "            stem: str = word\n",
    "\n",
    "            for pos_neg in positive_negatives:\n",
    "                for time in times:\n",
    "                    for person in people:\n",
    "                        seq: java.util.ArrayList = java.util.ArrayList()\n",
    "                        if pos_neg:\n",
    "                            seq.add(JString(pos_neg))\n",
    "                        if time:\n",
    "                            seq.add(JString(time))\n",
    "                        if person:\n",
    "                            seq.add(JString(person))\n",
    "                        results = list(morphology.getWordGenerator().generate(JString(stem),seq))\n",
    "                        if not results:\n",
    "                            continue\n",
    "            #             print(' '.join(str(result.surface) for result in results))\n",
    "                        for item in results:\n",
    "    #                         print(str(item.surface))\n",
    "                            ret_list.append(str(item.surface))\n",
    "        return ret_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If the data set is in a different path change news_path variable**\n",
    "\n",
    "**If zemberek jar file is in different path change zemberek_path variable**\n",
    "\n",
    "**read_doc is read and clear from noises words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_path = '1150haber'\n",
    "zemberek_path = 'zemberek-full.jar'\n",
    "\n",
    "create_corpus = CreateCorpus(zemberek_path)\n",
    "filenames = create_corpus.read_filenames(news_path)\n",
    "read_doc = create_corpus.read_files(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyzed word is analyzed word inner list with the form of [word,type]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzed_words = list()\n",
    "for item in read_doc:\n",
    "    result = create_corpus.analyze_word(item)\n",
    "    if len(result) == 2:\n",
    "        analyzed_words.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This part is optional. If you need to examine analyzed words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_text(content,filename):\n",
    "    with open(filename,'w') as f:\n",
    "        for i in content:\n",
    "            f.write(i[0])\n",
    "            f.write(',')\n",
    "            f.write(i[1])\n",
    "            f.write('\\n')\n",
    "\n",
    "write_text(analyzed_words,'new_corpusV2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generate(object):\n",
    "    word_size_dictionary = dict()\n",
    "    def __init__(self):\n",
    "        print(\"Generate object created\")\n",
    "    def word_size(self,word):\n",
    "        alphabet = {'a':1,'b':2,'c':3,'ç':4,'d':5,'e':6,'f':7,\n",
    "                'g':8,'ğ':9,'h':10,'ı':11,'i':12,'j':13,\n",
    "                'k':14,'l':15,'m':16,'n':17,'o':18,'ö':19,\n",
    "                'p':20,'r':21,'s':22,'ş':23,'t':24,'u':25,\n",
    "                'ü':26, 'v':27, 'y':28, 'z':29}\n",
    "        sum = 0\n",
    "        for letter in word:\n",
    "            if letter in alphabet:\n",
    "                sum += alphabet[letter]\n",
    "        return sum\n",
    "    \n",
    "    def generate_word(self,word_len):\n",
    "        chosen_words = list()\n",
    "        for key,value in self.word_size_dictionary.items():\n",
    "            if value == word_len:\n",
    "                chosen_words.append(key)\n",
    "        return random.choice(chosen_words)\n",
    "    \n",
    "    def create_sentence(self,arr,sentence_len):\n",
    "        noun = list()\n",
    "        verb = list()\n",
    "        adj = list()\n",
    "        adv = list()\n",
    "        num = list()\n",
    "        pron = list()\n",
    "        conj = list()\n",
    "        for a in arr:\n",
    "            if a[1] == 'Noun':\n",
    "                noun.append(a[0])\n",
    "            elif a[1] == 'Verb':\n",
    "                verb.append(a[0])\n",
    "            elif a[1] == \"Adv\":\n",
    "                adv.append(a[0])\n",
    "            elif a[1] == \"Adj\":\n",
    "                adj.append(a[0])\n",
    "            elif a[1] == \"Num\" or a[1] == \"Nu\":\n",
    "                num.append(a[0])\n",
    "            elif a[1] == \"Det\":\n",
    "                pron.append(a[0])\n",
    "            elif a[1] == \"Conj\":\n",
    "                conj.append(a[0])\n",
    "\n",
    "        a = True\n",
    "\n",
    "        value = sentence_len\n",
    "\n",
    "\n",
    "        if 50 >= value or value >= 800:\n",
    "            print(\"This program can create sentences with a total value between 800 and 50!\")\n",
    "\n",
    "\n",
    "        elif (value) > 50 and (value) < 350:\n",
    "            while a:\n",
    "                indexP = randint(0, len(pron))\n",
    "                valP = self.word_size(pron[indexP])\n",
    "\n",
    "                indexN = randint(0, len(noun))\n",
    "                valN = self.word_size(noun[indexN])\n",
    "\n",
    "                valV = value - (valP + valN)\n",
    "                if value < 0:\n",
    "                    continue\n",
    "\n",
    "                verbWord = \"\"\n",
    "\n",
    "                for i in verb:\n",
    "                    if self.word_size(i) == valV:\n",
    "                        verbWord = i\n",
    "                        print(\"Pronoun :\", pron[indexP], valP)\n",
    "                        print(\"Noun :\", noun[indexN], valN)\n",
    "                        print(\"Verb :\", i, valV)\n",
    "                        print(\"-->\", pron[indexP], noun[indexN], i)\n",
    "                        break\n",
    "                if verbWord != \"\":\n",
    "                    a = False\n",
    "        elif value >= 350 and value < 450:\n",
    "            while a:\n",
    "                indexP = randint(0, len(pron))\n",
    "                valP = self.word_size(pron[indexP])\n",
    "\n",
    "                indexN = randint(0, len(noun))\n",
    "                valN = self.word_size(noun[indexN])\n",
    "\n",
    "                indexAj = randint(0, len(adj))\n",
    "                valAj = self.word_size(adj[indexAj])\n",
    "\n",
    "                valV = value - (valP + valN + valAj)\n",
    "                if value < 0:\n",
    "                    continue\n",
    "\n",
    "                verbWord = \"\"\n",
    "\n",
    "                for i in verb:\n",
    "                    if self.word_size(i) == valV:\n",
    "                        verbWord = i\n",
    "                        print(\"Pronoun :\", pron[indexP], valP)\n",
    "                        print(\"Adjective :\", adj[indexAj], valAj)\n",
    "                        print(\"Noun :\", noun[indexN], valN)\n",
    "                        print(\"Verb :\", i, valV)\n",
    "                        print(\"-->\", pron[indexP], adj[indexAj], noun[indexN], i)\n",
    "                        break\n",
    "                if verbWord != \"\":\n",
    "                    a = False\n",
    "        elif value >= 450 and value < 550:\n",
    "            while a:\n",
    "                indexP = randint(0, len(pron))\n",
    "                valP = self.word_size(pron[indexP])\n",
    "\n",
    "                indexN = randint(0, len(noun))\n",
    "                valN = self.word_size(noun[indexN])\n",
    "\n",
    "                indexAj = randint(0, len(adj))\n",
    "                valAj = self.word_size(adj[indexAj])\n",
    "\n",
    "                indexNm = randint(0, len(num))\n",
    "                valNm = self.word_size(num[indexNm])\n",
    "\n",
    "                valV = value - (valP + valN + valAj + valNm)\n",
    "\n",
    "                verbWord = \"\"\n",
    "                if value < 0:\n",
    "                    continue\n",
    "\n",
    "                for i in verb:\n",
    "                    if self.word_size(i) == valV:\n",
    "                        verbWord = i\n",
    "                        print(\"Pronoun :\", pron[indexP], valP)\n",
    "                        print(\"Number :\", num[indexNm], valNm)\n",
    "                        print(\"Adjective: \", adj[indexAj], valAj)\n",
    "                        print(\"Noun :\", noun[indexN], valN)\n",
    "                        print(\"Verb :\", i, valV)\n",
    "                        print(\"-->\", pron[indexP], num[indexNm], adj[indexAj], noun[indexN], i)\n",
    "                        break\n",
    "                if verbWord != \"\":\n",
    "                    a = False\n",
    "\n",
    "        elif value >= 550 and value <= 650:\n",
    "            while a:\n",
    "                indexP = randint(0, len(pron))\n",
    "                valP = self.word_size(pron[indexP])\n",
    "\n",
    "                indexN = randint(0, len(noun))\n",
    "                valN = self.word_size(noun[indexN])\n",
    "\n",
    "                indexAj = randint(0, len(adj))\n",
    "                valAj = self.word_size(adj[indexAj])\n",
    "\n",
    "                indexNm = randint(0, len(num))\n",
    "                valNm = self.word_size(num[indexNm])\n",
    "\n",
    "                indexAv = randint(0, len(adv))\n",
    "                valAv = self.word_size(adv[indexAv])\n",
    "\n",
    "                valV = value - (valP + valN + valAj + valNm + valAv)\n",
    "                if value < 0:\n",
    "                    continue\n",
    "\n",
    "                verbWord = \"\"\n",
    "\n",
    "                for i in verb:\n",
    "                    if self.word_size(i) == valV:\n",
    "                        verbWord = i\n",
    "                        print(\"Pronoun : \", pron[indexP], valP)\n",
    "                        print(\"Number : \", num[indexNm], valNm)\n",
    "                        print(\"Adjective : \", adj[indexAj], valAj)\n",
    "                        print(\"Noun : \", noun[indexN], valN)\n",
    "                        print(\"Adverb : \", adv[indexAv], valAv)\n",
    "                        print(\"Verb : \", i, \" \", valV)\n",
    "                        print(\"-->\", pron[indexP], num[indexNm], adj[indexAj], noun[indexN], adv[indexAv], i)\n",
    "                        break\n",
    "                if verbWord != \"\":\n",
    "                    a = False\n",
    "\n",
    "        elif value >= 650 and value < 800:\n",
    "            while a:\n",
    "                indexP = randint(0, len(pron))\n",
    "                valP = self.word_size(pron[indexP])\n",
    "\n",
    "                indexN1 = randint(0, len(noun))\n",
    "                valN1 = self.word_size(noun[indexN1])\n",
    "\n",
    "                indexC = randint(0, len(conj))\n",
    "                valC = self.word_size(conj[indexC])\n",
    "\n",
    "                indexN2 = randint(0, len(noun))\n",
    "                valN2 = self.word_size(noun[indexN2])\n",
    "\n",
    "                indexAj = randint(0, len(adj))\n",
    "                valAj = self.word_size(adj[indexAj])\n",
    "\n",
    "                indexNm = randint(0, len(num))\n",
    "                valNm = self.word_size(num[indexNm])\n",
    "\n",
    "                indexAv = randint(0, len(adv))\n",
    "                valAv = self.word_size(adv[indexAv])\n",
    "\n",
    "                valV = value - (valP + valN1 + valC + valN2 + valAj + valNm + valAv)\n",
    "                if value < 0:\n",
    "                    continue\n",
    "\n",
    "                verbWord = \"\"\n",
    "\n",
    "                for i in verb:\n",
    "                    if self.word_size(i) == valV:\n",
    "                        verbWord = i\n",
    "                        print(\"Pronoun :\", pron[indexP], valP)\n",
    "                        print(\"Number :\", num[indexNm], valNm)\n",
    "                        print(\"Adjective :\", adj[indexAj], valAj)\n",
    "                        print(\"Noun :\", noun[indexN1], valN1)\n",
    "                        print(\"Conj :\", conj[indexC], valC)\n",
    "                        print(\"Noun :\", noun[indexN2], valN2)\n",
    "                        print(\"Adverb :\", adv[indexAv], valAv)\n",
    "                        print(\"Verb :\", i, valV)\n",
    "                        print(\"-->\", pron[indexP], num[indexNm], adj[indexAj], noun[indexN1], conj[indexC], noun[indexN2],\n",
    "                              adv[indexAv], i)\n",
    "                        break\n",
    "                if verbWord != \"\":\n",
    "                    a = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate object created\n"
     ]
    }
   ],
   "source": [
    "generate = Generate()\n",
    "for word in read_doc:\n",
    "    generate.word_size_dictionary[word] = generate.word_size(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you want to generate different word with different lenghts, change word_size variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'setlere'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_size = 100\n",
    "generate.generate_word(word_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This part is check word size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate.word_size('okan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you want to generate different sentences with different length, change sentence_value variable**\n",
    "\n",
    "**If it took too long, stop this cell and please re-run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pronoun : bu 27\n",
      "Noun : konferans 123\n",
      "Verb : göster 100\n",
      "--> bu konferans göster\n"
     ]
    }
   ],
   "source": [
    "sentence_value = 250\n",
    "generate.create_sentence(analyzed_words,sentence_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
